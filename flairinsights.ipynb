{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLiEAujXcUDm"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import csv\n",
        "\n",
        "def scrape_web_page(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the web page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract relevant information such as text, images, or links\n",
        "        # For this example, let's extract the text from all paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        extracted_text = '\\n'.join([p.text.strip() for p in paragraphs])\n",
        "\n",
        "        return extracted_text\n",
        "    else:\n",
        "        print(\"Failed to fetch the web page. Status code:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data, output_file):\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['Extracted Text'])\n",
        "        writer.writerow([data])\n",
        "\n",
        "def save_to_json(data, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump({'Extracted_Text': data}, jsonfile, indent=4)\n",
        "\n",
        "def main():\n",
        "    # URL of the web page to scrape\n",
        "    url = 'https://pubchem.ncbi.nlm.nih.gov/compound/N-Vinyl-2-pyrrolidone'  # Replace with the URL of the attached web page\n",
        "\n",
        "    # Output file paths\n",
        "    output_csv_file = 'output.csv'\n",
        "    output_json_file = 'output.json'\n",
        "\n",
        "    # Scrape the web page\n",
        "    extracted_text = scrape_web_page(url)\n",
        "\n",
        "    if extracted_text:\n",
        "        # Save to CSV\n",
        "        save_to_csv(extracted_text, output_csv_file)\n",
        "\n",
        "        # Save to JSON\n",
        "        save_to_json(extracted_text, output_json_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "\n",
        "# Function to scrape data\n",
        "def scrape_data(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Initialize lists to store extracted data\n",
        "        extracted_data = []\n",
        "\n",
        "        # Example: Extracting text from paragraphs with class 'content'\n",
        "        paragraphs = soup.find_all('p', class_='content')\n",
        "        for paragraph in paragraphs:\n",
        "            extracted_data.append({\n",
        "                'text': paragraph.get_text(strip=True)\n",
        "            })\n",
        "\n",
        "        # Example: Extracting image URLs from img tags\n",
        "        images = soup.find_all('img')\n",
        "        for image in images:\n",
        "            # Check if the 'src' attribute exists before accessing it\n",
        "            if 'src' in image.attrs:\n",
        "                extracted_data.append({\n",
        "                    'image_url': image['src']\n",
        "                })\n",
        "\n",
        "        # Example: Extracting links\n",
        "        links = soup.find_all('a', href=True)\n",
        "        for link in links:\n",
        "            extracted_data.append({\n",
        "                'link_text': link.text,\n",
        "                'link_url': link['href']\n",
        "            })\n",
        "\n",
        "        return extracted_data\n",
        "    else:\n",
        "        print(\"Error: Failed to retrieve webpage.\")\n",
        "        return None\n",
        "\n",
        "# Function to save data to CSV\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = data[0].keys() if data else []\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for item in data:\n",
        "            writer.writerow(item)\n",
        "\n",
        "# Function to save data to JSON\n",
        "def save_to_json(data, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(data, jsonfile, indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    url = 'https://products.basf.com/global/en/ci/n-vinyl-2-pyrrolidone.html'\n",
        "    data = scrape_data(url)\n",
        "\n",
        "    if data:\n",
        "        # Save data to CSV\n",
        "        save_to_csv(data, 'scraped_data.csv')\n",
        "\n",
        "        # Save data to JSON\n",
        "        save_to_json(data, 'scraped_data.json')\n",
        "\n",
        "        print(\"Scraping and saving successful.\")\n",
        "    else:\n",
        "        print(\"Scraping failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBoaEfoQM5FY",
        "outputId": "70e4bbc8-8d7b-461f-ad72-4d8243d5f0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping and saving successful.\n"
          ]
        }
      ]
    }
  ]
}